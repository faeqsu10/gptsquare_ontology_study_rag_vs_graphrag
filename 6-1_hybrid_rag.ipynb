{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76911f3a",
   "metadata": {},
   "source": [
    "### 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2c20da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Optional, Any\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0f914",
   "metadata": {},
   "source": [
    "### 시스템 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d01ee85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma\n",
    "CHROMA_DB_PATH = \"./chroma_db\" \n",
    "\n",
    "# Neo4j\n",
    "USERNAME = os.environ.get(\"USERNAME\")\n",
    "PASSWORD = os.environ.get(\"PASSWORD\")\n",
    "URL = os.environ.get(\"URL\")\n",
    "\n",
    "# OpenAI\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-5-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2291fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40f556",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 프롬프트\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "당신은 Bella Roma 레스토랑 데이터 전문가를 위한 유용한 AI 어시스턴트입니다.\n",
    "제공된 Context (검색된 문서)만을 사용하여 사용자의 질문에 답변하십시오.\n",
    "만약 Context에 답변할 수 있는 정보가 없다면, '제공된 정보로는 답변할 수 없습니다.'라고 명확하게 언급하세요.\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "[Context (검색된 문서)]\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12428ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB 초기화\n",
    "\n",
    "def initialize_chroma_db(embedding_function, collection_name=\"unified_data\"):\n",
    "    \"\"\" ChromaDB 벡터 스토어를 초기화합니다. (가정: 데이터는 이미 로드되어 있습니다) \"\"\"\n",
    "    try:\n",
    "        # 이미 데이터가 존재하는 컬렉션에 연결합니다.\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=CHROMA_DB_PATH,\n",
    "            embedding_function=embedding_function,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        \n",
    "        collection = getattr(vector_db, \"_collection\", None)\n",
    "        count = collection.count()\n",
    "        print(f\"연결된 컬렉션(unified_data) 문서 개수: {count}\")\n",
    "        print()\n",
    "        \n",
    "        return vector_db\n",
    "    except Exception as e:\n",
    "        print(f\"ChromaDB 초기화 오류: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ffb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연결된 컬렉션(unified_data) 문서 개수: 608\n",
      "RAG 답변: 주중(월~금) 영업시간은 11:00부터 22:00까지입니다.  \n",
      "결제 수단은 현금, 카드 및 다양한 간편결제를 지원합니다.\n"
     ]
    }
   ],
   "source": [
    "# RAG 파이프라인\n",
    "\n",
    "def rag_pipeline(question: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Vector DB (ChromaDB)를 사용하여 비정형 데이터 기반 RAG 파이프라인을 LCEL로 실행합니다.\n",
    "    \"\"\"\n",
    "    # 1. Vector DB 초기화 및 임베딩 함수 설정\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vector_db = initialize_chroma_db(embeddings, collection_name=\"unified_data\")\n",
    "    \n",
    "    if vector_db is None:\n",
    "        return \"ERROR: Vector DB를 초기화할 수 없습니다.\"\n",
    "\n",
    "    # 2. Retriever 및 Prompt Template 정의\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "    prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "    # 3. LCEL 체인 구성 (RunnablePassthrough 사용)\n",
    "    # 체인 흐름: \n",
    "    # {context: 검색(retriever), question: 질문 원문(RunnablePassthrough)} \n",
    "    # -> Prompt -> LLM -> 결과 파싱\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # 4. 질문 실행 및 결과 생성\n",
    "    try:\n",
    "        # invoke 메서드는 LCEL 체인을 실행하며, 입력은 question 문자열입니다.\n",
    "        result = rag_chain.invoke(question)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: RAG 파이프라인 실행 오류: {e}\"\n",
    "\n",
    "# 예시 사용법 (실제 실행 시 주석 해제)\n",
    "question = \"Bella Roma의 주중 운영 시간은 무엇이며, 어떤 결제 수단을 받나요?\"\n",
    "# question = \"Bella Roma 레스토랑의 영업시간을 알려주세요.\"\n",
    "# llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0) # llm 객체 초기화 필요\n",
    "answer = rag_pipeline(question, llm)\n",
    "print(f\"RAG 답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1d4f8",
   "metadata": {},
   "source": [
    "### Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ba2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_neo4j_graph() -> Neo4jGraph:\n",
    "    \"\"\" Neo4jGraph 객체를 초기화합니다. \"\"\"\n",
    "    # 실제 환경에서는 NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD를 사용해야 합니다.\n",
    "    try:\n",
    "        # 임시 URL/인증 정보를 가정하거나 환경 변수에서 로드합니다.\n",
    "        graph = Neo4jGraph(\n",
    "            url=\"bolt://52.3.233.24:7687\", # 실제 URL로 변경 필요\n",
    "            username=\"neo4j\", \n",
    "            password=\"canvases-return-armaments\"\n",
    "        )\n",
    "        # 스키마가 미리 로드되어 있다고 가정\n",
    "        return graph\n",
    "    except Exception as e:\n",
    "        print(f\"Neo4j 초기화 오류: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "320056ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_rag_pipeline(question: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Graph DB (Neo4j)를 사용하여 스키마 기반 Cypher 쿼리 생성 파이프라인을 실행합니다.\n",
    "    \"\"\"\n",
    "    # 1. Neo4j Graph 초기화\n",
    "    graph = initialize_neo4j_graph()\n",
    "    if graph is None:\n",
    "        return \"ERROR: Neo4j Graph를 초기화할 수 없습니다.\"\n",
    "    \n",
    "    # 2. GraphCypherQAChain 생성\n",
    "    # LLM에 스키마를 전달하여 Cypher 쿼리 생성 및 실행 후 답변 생성\n",
    "    qa_chain = GraphCypherQAChain.from_llm(\n",
    "        llm=llm,\n",
    "        graph=graph,\n",
    "        verbose=False,\n",
    "        allow_dangerous_requests=True,\n",
    "        # LLM이 쿼리를 생성하고 결과를 자연어로 해석할 때 스키마가 Context로 사용됨\n",
    "    )\n",
    "    \n",
    "    # 3. 질문 실행 및 결과 생성\n",
    "    try:\n",
    "        result = qa_chain.invoke({\"query\": question})\n",
    "        return result[\"result\"]\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: GRAPH RAG 파이프라인 실행 오류: {e}\"\n",
    "\n",
    "# # 예시 사용법 (실제 실행 시 주석 해제)\n",
    "# question = \"마르게리타 피자는 어떤 재료를 포함하며, 유제품 알러지를 유발하나요?\"\n",
    "# answer = graph_rag_pipeline(question, llm)\n",
    "# print(f\"GRAPH RAG 답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa78f6f",
   "metadata": {},
   "source": [
    "### Hybrid (RAG & Graph RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81265d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이브리드 RAG 프롬프트\n",
    "\n",
    "HYBRID_QA_TEMPLATE = \"\"\"\n",
    "당신은 Bella Roma 레스토랑 데이터 분석 전문가입니다.\n",
    "아래에 제공된 두 가지 검색 결과 (정형 데이터 및 비정형 데이터)를 모두 활용하여 사용자의 질문에 대한 가장 정확하고 포괄적인 답변을 한국어로 생성하십시오.\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "--- Graph DB (정형 데이터) Context ---\n",
    "{graph_context}\n",
    "---------------------------------------\n",
    "\n",
    "--- Vector DB (비정형 데이터) Context ---\n",
    "{vector_context}\n",
    "-----------------------------------------\n",
    "\n",
    "# 응답 가이드라인:\n",
    "- 두 Context를 종합하여 답변하며, 충돌하는 정보가 있을 경우 정형 데이터(Graph DB)의 사실 정보를 우선하세요.\n",
    "- 답변은 전문적이고 유익한 톤을 사용하세요.\n",
    "- 두 Context 모두에 답변할 정보가 없는 경우, '제공된 정보로는 답변을 통합할 수 없습니다.'라고 명확히 언급하세요.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14df58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYBRID_QA_PROMPT = ChatPromptTemplate.from_template(HYBRID_QA_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chroma_db(embedding_function: OpenAIEmbeddings, collection_name: str) -> Optional[Chroma]:\n",
    "    \"\"\" ChromaDB 벡터 스토어를 초기화합니다. \"\"\"\n",
    "    try:\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=CHROMA_DB_PATH,\n",
    "            embedding_function=embedding_function,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        # print(f\"연결된 컬렉션({collection_name}) 문서 개수: {getattr(vector_db, '_collection').count()}\")\n",
    "        return vector_db\n",
    "    except Exception as e:\n",
    "        print(f\"ChromaDB 초기화 오류: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f51761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_neo4j_graph() -> Optional[Neo4jGraph]:\n",
    "    \"\"\" Neo4jGraph 객체를 초기화합니다. \"\"\"\n",
    "    try:\n",
    "        graph = Neo4jGraph(url=URL, username=USERNAME, password=PASSWORD)\n",
    "        return graph\n",
    "    except Exception as e:\n",
    "        print(f\"Neo4j 초기화 오류: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8096cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_vector_context(question: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\" Vector DB에서 관련 문서의 Context를 검색합니다. \"\"\"\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        vector_db = initialize_chroma_db(embeddings, collection_name=\"unified_data\")\n",
    "        \n",
    "        if vector_db is None:\n",
    "            return \"ERROR: Vector DB 초기화 실패\"\n",
    "        \n",
    "        # 5개 문서를 검색하여 텍스트 Context로 변환\n",
    "        retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "        docs = retriever.invoke(question)\n",
    "        \n",
    "        # 검색된 문서 내용을 하나의 문자열로 결합\n",
    "        context = \"\\n---\\n\".join([doc.page_content for doc in docs])\n",
    "        return context\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: Vector DB 검색 오류: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7d103a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_graph_context(question: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\" Graph DB에서 Cypher 쿼리를 생성 및 실행하여 결과를 반환합니다. \"\"\"\n",
    "    graph = initialize_neo4j_graph()\n",
    "    if graph is None:\n",
    "        return \"ERROR: Neo4j Graph 초기화 실패\"\n",
    "    \n",
    "    # GraphCypherQAChain 생성 및 실행\n",
    "    qa_chain = GraphCypherQAChain.from_llm(\n",
    "        llm=llm,\n",
    "        graph=graph,\n",
    "        verbose=False,\n",
    "        allow_dangerous_requests=True,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # LLM은 Cypher 쿼리 실행 결과를 바탕으로 자연어 Context를 생성하여 반환합니다.\n",
    "        result = qa_chain.invoke({\"query\": question})\n",
    "        \n",
    "        # GraphCypherQAChain의 결과는 이미 자연어 답변 형태이지만,\n",
    "        # 여기서는 LLM이 최종 답변을 만들기 위한 'Context'로 사용되므로,\n",
    "        # Cypher 쿼리와 결과를 모두 포함하는 디버그 정보로 전달하는 것이 이상적입니다.\n",
    "        # (편의상 여기서는 chain의 최종 result만 사용합니다.)\n",
    "        return result[\"result\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: GRAPH RAG 실행 오류: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이브리드 파이프라인\n",
    "\n",
    "def hybrid_rag_pipeline(question: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Graph DB (정형)와 Vector DB (비정형)의 Context를 결합하여 최종 답변을 생성합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 1. Graph DB 컨텍스트 검색 시작 ---\")\n",
    "    # (a) Graph DB 검색 - 정형 데이터 및 관계 정보\n",
    "    graph_context = fetch_graph_context(question, llm)\n",
    "    print(f\"Graph Context: {graph_context[:100]}...\")\n",
    "\n",
    "    print(\"--- 2. Vector DB 컨텍스트 검색 시작 ---\")\n",
    "    # (b) Vector DB 검색 - 비정형 텍스트 문서\n",
    "    vector_context = fetch_vector_context(question, llm)\n",
    "    print(f\"Vector Context: {vector_context[:100]}...\")\n",
    "\n",
    "    # 3. Context 결합 및 최종 답변 생성 (LCEL 사용)\n",
    "    \n",
    "    # Context를 결합하는 Runnable\n",
    "    combined_context_runnable = RunnableLambda(\n",
    "        lambda q: {\n",
    "            \"question\": q, \n",
    "            \"graph_context\": graph_context, \n",
    "            \"vector_context\": vector_context\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # LCEL 체인 구성: 입력 -> Context 결합 -> Prompt -> LLM -> 결과 파싱\n",
    "    hybrid_chain = (\n",
    "        combined_context_runnable\n",
    "        | HYBRID_QA_PROMPT\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    print(\"--- 3. 하이브리드 LLM 추론 시작 ---\")\n",
    "    try:\n",
    "        final_answer = hybrid_chain.invoke(question)\n",
    "        return final_answer\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: 하이브리드 LLM 추론 오류: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c7056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Graph DB 컨텍스트 검색 시작 ---\n",
      "Graph Context: 2024년 8월에 가장 많이 팔린 메뉴는 까르보나라 파스타로, 총 35개가 판매되었습니다....\n",
      "--- 2. Vector DB 컨텍스트 검색 시작 ---\n",
      "Vector Context: 사용자 사용자8이 2024-10-03에 평점 3점으로 '분위기가 좋아서 데이트 장소로 딱입니다.'라는 리뷰를 남겼습니다.\n",
      "---\n",
      "사용자 사용자4이 2024-10-06에 평점 4점으...\n",
      "--- 3. 하이브리드 LLM 추론 시작 ---\n",
      "\n",
      "=== 최종 하이브리드 RAG 답변 ===\n",
      "2024년 8월에 가장 많이 팔린 메뉴는 까르보나라 파스타로, 총 35개가 판매되었습니다. 이 메뉴는 고객들 사이에서 인기가 높아 데이트 장소로도 적합하다는 긍정적인 리뷰가 여러 건 남겨졌습니다. 예를 들어, 사용자8과 사용자4는 각각 '분위기가 좋아서 데이트 장소로 딱입니다.'라는 리뷰를 남겼습니다. 이러한 리뷰는 까르보나라 파스타가 맛뿐만 아니라 레스토랑의 분위기와도 잘 어울린다는 점을 강조합니다. \n",
      "\n",
      "따라서, 2024년 8월의 판매 데이터와 고객 리뷰를 종합적으로 고려할 때, 까르보나라 파스타는 고객들에게 매우 긍정적인 반응을 얻고 있는 메뉴임을 알 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 실행 예시 (사용자가 직접 실행) \n",
    "\n",
    "question_hybrid = \"2024년 8월에 가장 많이 팔린 메뉴는 무엇인가요?\"\n",
    "llm_instance = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0) \n",
    "answer = hybrid_rag_pipeline(question_hybrid, llm_instance)\n",
    "print(\"\\n=== 최종 하이브리드 RAG 답변 ===\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT Square Ontology",
   "language": "python",
   "name": "gptsquare_ontology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
