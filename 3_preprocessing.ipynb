{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92428eaa",
   "metadata": {},
   "source": [
    "---\n",
    "### 데이터 통합 처리 파이프라인\n",
    "\n",
    "- **데이터 로드:**  \n",
    "  - `data/` 폴더 내의 주요 마크다운 파일(`식당정보.md`, `직원.md`, `메뉴.md`, `공급업체.md`)을 UTF-8로 읽어옵니다.\n",
    "  - `csv_output` 폴더의 `4데이터_구매이력.csv`(구매이력), `5데이터_리뷰.csv`(리뷰) 파일을 판다스의 `read_csv`로 불러옵니다.\n",
    "\n",
    "- **전처리 및 청크 분할:**  \n",
    "  - 마크다운 파일은 각 파일별로 600자 단위, 100자 겹치기로 텍스트를 분할합니다.\n",
    "  - 각 청크에는 데이터 타입, 파일명, 청크 인덱스 등 메타데이터를 추가합니다.\n",
    "  - 구매이력과 리뷰 데이터는 각 행을 자연어 문장으로 변환하고, 관련 메타데이터(유저, 날짜, 메뉴, 평점 등)를 함께 저장합니다.\n",
    "\n",
    "- **최종 통합 및 저장:**  \n",
    "  - 모든 텍스트 청크와 문장, 메타데이터, 고유 ID를 리스트로 통합하여 반환합니다.\n",
    "  - 통합된 데이터를 크로마DB(ChromaDB) 컬렉션에 저장합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da11be04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658017e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chunk_size=500, overlap_size=50):\n",
    "    \"\"\"langchain을 사용하여 텍스트를 청크로 분할\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_size,\n",
    "        chunk_overlap=overlap_size,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \"]\n",
    "    )\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc0c6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chromadb():\n",
    "    \"\"\"ChromaDB 클라이언트 설정\"\"\"\n",
    "    # \"chroma_db\" 라는 이름의 폴더에 데이터베이스를 저장하도록 설정\n",
    "    client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "        model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return client, openai_ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0eaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_stats(collection):\n",
    "    \"\"\"컬렉션 통계 정보 출력\"\"\"\n",
    "    results = collection.get()\n",
    "    \n",
    "    total_docs = len(results['ids'])\n",
    "    data_types = {}\n",
    "    \n",
    "    for metadata in results['metadatas']:\n",
    "        data_type = metadata.get('data_type', 'unknown')\n",
    "        data_types[data_type] = data_types.get(data_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\n컬렉션: {collection.name}\")\n",
    "    print(f\"총 문서 수: {total_docs}\")\n",
    "    print(\"데이터 타입별 분포:\")\n",
    "    for data_type, count in data_types.items():\n",
    "        print(f\"  {data_type}: {count}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49ef279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_chromadb(client, openai_ef, collection_name, documents, metadatas, ids):\n",
    "    \"\"\"ChromaDB에 데이터 삽입\"\"\"\n",
    "    print(f\"{collection_name} 컬렉션에 데이터 삽입 중...\")\n",
    "    \n",
    "    # 기존 컬렉션 삭제 (있다면)\n",
    "    try:\n",
    "        client.delete_collection(collection_name)\n",
    "    except Exception:\n",
    "        pass  # 컬렉션이 없으면 무시\n",
    "    \n",
    "    # 새 컬렉션 생성\n",
    "    collection = client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=openai_ef\n",
    "    )\n",
    "    \n",
    "    # 데이터 삽입\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    \n",
    "    print(f\"{collection_name} 삽입 완료: {len(documents)}개 문서\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ace7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_data(input_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    모든 데이터 통합 처리\n",
    "    \"\"\"\n",
    "    print(\"모든 데이터 통합 처리 중...\")\n",
    "    \n",
    "    all_documents = []\n",
    "    all_metadatas = []\n",
    "    all_ids = []\n",
    "    \n",
    "    # 1. 마크다운 파일들 처리\n",
    "    print(\"마크다운 파일들 처리 중...\")\n",
    "    files_config = {\n",
    "        \"식당정보.md\": \"restaurant_info\",\n",
    "        \"직원.md\": \"employee_info\", \n",
    "        \"메뉴.md\": \"menu_info\",\n",
    "        \"공급업체.md\": \"supplier_info\"\n",
    "    }\n",
    "    \n",
    "    for filename, data_type in files_config.items():\n",
    "        try:\n",
    "            with open(Path(input_dir) / filename, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            chunks = chunk_text(content, max_chunk_size=600, overlap_size=100)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                all_documents.append(chunk)\n",
    "                all_metadatas.append({\n",
    "                    \"data_type\": data_type,\n",
    "                    \"filename\": filename,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks)\n",
    "                })\n",
    "                all_ids.append(f\"{data_type}_{i}\")\n",
    "            \n",
    "            print(f\"{filename} 처리 완료: {len(chunks)}개 청크\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"{filename} 파일을 찾을 수 없습니다.\")\n",
    "            continue\n",
    "    \n",
    "    # 2. 구매이력 데이터 처리\n",
    "    print(\"구매이력 데이터 처리 중...\")\n",
    "    try:\n",
    "        df_purchase = pd.read_csv(Path(input_dir) / \"csv_output\" / \"4데이터_구매이력.csv\")\n",
    "        print(f\"총 {len(df_purchase)}개 구매 레코드 로드\")\n",
    "        \n",
    "        for idx, row in df_purchase.iterrows():\n",
    "            purchase_text = f\"사용자 {row['User']}이 {row['Date']}에 {row['MenuItem']} {row['Quantity']}개를 {row['PricePerUnit']:,}원에 구매하여 총 {row['TotalPrice']:,}원을 지불했습니다.\"\n",
    "            \n",
    "            all_documents.append(purchase_text)\n",
    "            all_metadatas.append({\n",
    "                \"data_type\": \"purchase\",\n",
    "                \"user\": row['User'],\n",
    "                \"menu_item\": row['MenuItem'],\n",
    "                \"quantity\": int(row['Quantity']),\n",
    "                \"price_per_unit\": int(row['PricePerUnit']),\n",
    "                \"total_price\": int(row['TotalPrice']),\n",
    "                \"date\": row['Date']\n",
    "            })\n",
    "            all_ids.append(f\"purchase_{idx}\")\n",
    "        \n",
    "        print(f\"구매이력 데이터 처리 완료: {len(df_purchase)}개 문서\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"구매이력 CSV 파일을 찾을 수 없습니다.\")\n",
    "    \n",
    "    # 3. 리뷰 데이터 처리\n",
    "    print(\"리뷰 데이터 처리 중...\")\n",
    "    try:\n",
    "        df_review = pd.read_csv(Path(input_dir) / \"csv_output\" / \"5데이터_리뷰.csv\")\n",
    "        print(f\"총 {len(df_review)}개 리뷰 레코드 로드\")\n",
    "        \n",
    "        for idx, row in df_review.iterrows():\n",
    "            review_text = f\"사용자 {row['User']}이 {row['Date']}에 평점 {row['Rating']}점으로 '{row['Review']}'라는 리뷰를 남겼습니다.\"\n",
    "            \n",
    "            all_documents.append(review_text)\n",
    "            all_metadatas.append({\n",
    "                \"data_type\": \"review\",\n",
    "                \"user\": row['User'],\n",
    "                \"rating\": int(row['Rating']),\n",
    "                \"review_text\": row['Review'],\n",
    "                \"date\": row['Date']\n",
    "            })\n",
    "            all_ids.append(f\"review_{idx}\")\n",
    "        \n",
    "        print(f\"리뷰 데이터 처리 완료: {len(df_review)}개 문서\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"리뷰 CSV 파일을 찾을 수 없습니다.\")\n",
    "    \n",
    "    print(f\"전체 데이터 처리 완료: {len(all_documents)}개 문서\")\n",
    "    return all_documents, all_metadatas, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b3277bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    print(\"데이터 처리 및 벡터DB 로딩 시작\\n\")\n",
    "    \n",
    "    # ChromaDB 설정\n",
    "    client, openai_ef = setup_chromadb()\n",
    "    \n",
    "    # 모든 데이터 통합 처리\n",
    "    print(\"=\" * 50)\n",
    "    print(\"모든 데이터 통합 처리\")\n",
    "    print(\"=\" * 50)\n",
    "    all_docs, all_metas, all_ids = process_all_data()\n",
    "    \n",
    "    # 하나의 컬렉션에 저장\n",
    "    unified_collection = insert_to_chromadb(\n",
    "        client, openai_ef, \"unified_data\", \n",
    "        all_docs, all_metas, all_ids\n",
    "    )\n",
    "    get_collection_stats(unified_collection)\n",
    "    \n",
    "    print(\"\\n모든 작업 완료!\")\n",
    "    print(f\"통합 컬렉션: {unified_collection.count()}개 문서\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c56d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 처리 및 벡터DB 로딩 시작\n",
      "\n",
      "==================================================\n",
      "모든 데이터 통합 처리\n",
      "==================================================\n",
      "모든 데이터 통합 처리 중...\n",
      "마크다운 파일들 처리 중...\n",
      "식당정보.md 처리 완료: 1개 청크\n",
      "직원.md 처리 완료: 2개 청크\n",
      "메뉴.md 처리 완료: 2개 청크\n",
      "공급업체.md 처리 완료: 1개 청크\n",
      "구매이력 데이터 처리 중...\n",
      "총 482개 구매 레코드 로드\n",
      "구매이력 데이터 처리 완료: 482개 문서\n",
      "리뷰 데이터 처리 중...\n",
      "총 120개 리뷰 레코드 로드\n",
      "리뷰 데이터 처리 완료: 120개 문서\n",
      "전체 데이터 처리 완료: 608개 문서\n",
      "unified_data 컬렉션에 데이터 삽입 중...\n",
      "unified_data 삽입 완료: 608개 문서\n",
      "\n",
      "컬렉션: unified_data\n",
      "총 문서 수: 608\n",
      "데이터 타입별 분포:\n",
      "  restaurant_info: 1개\n",
      "  employee_info: 2개\n",
      "  menu_info: 2개\n",
      "  supplier_info: 1개\n",
      "  purchase: 482개\n",
      "  review: 120개\n",
      "\n",
      "모든 작업 완료!\n",
      "통합 컬렉션: 608개 문서\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT Square Ontology",
   "language": "python",
   "name": "gptsquare_ontology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
