{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b096d1b9",
   "metadata": {},
   "source": [
    "### 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c20da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Optional, Any\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b923a1",
   "metadata": {},
   "source": [
    "### 시스템 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01ee85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma\n",
    "CHROMA_DB_PATH = \"./chroma_db\" \n",
    "\n",
    "# Neo4j\n",
    "USERNAME = os.environ.get(\"USERNAME\")\n",
    "PASSWORD = os.environ.get(\"PASSWORD\")\n",
    "URL = os.environ.get(\"URL\")\n",
    "\n",
    "# OpenAI\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-5-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d44fc1a",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 프롬프트\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "당신은 Bella Roma 레스토랑 데이터 전문가를 위한 유용한 AI 어시스턴트입니다.\n",
    "제공된 Context (검색된 문서)만을 사용하여 사용자의 질문에 답변하십시오.\n",
    "만약 Context에 답변할 수 있는 정보가 없다면, '제공된 정보로는 답변할 수 없습니다.'라고 명확하게 언급하세요.\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "[Context (검색된 문서)]\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12428ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chroma_db(embedding_function, collection_name=\"unified_data\"):\n",
    "    \"\"\" ChromaDB 벡터 스토어를 초기화합니다. (가정: 데이터는 이미 로드되어 있습니다) \"\"\"\n",
    "    try:\n",
    "        # 이미 데이터가 존재하는 컬렉션에 연결합니다.\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=CHROMA_DB_PATH,\n",
    "            embedding_function=embedding_function,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        \n",
    "        collection = getattr(vector_db, \"_collection\", None)\n",
    "        count = collection.count()\n",
    "        print(f\"연결된 컬렉션(unified_data) 문서 개수: {count}\")\n",
    "        print()\n",
    "        \n",
    "        return vector_db\n",
    "    except Exception as e:\n",
    "        print(f\"ChromaDB 초기화 오류: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed6ffb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Context: # Bella Roma 직원 정보\n",
      "\n",
      "## 팀원 소개\n",
      "\n",
      "이 문서에서는 Bella Roma 레스토랑을 이끌어가는 주요 직원들의 정보를 소개합니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 김철수 (주방장, Head Chef)\n",
      "\n",
      "-   **역할**: 주방장 (Head Chef)\n",
      "-   **경력**: 15년\n",
      "-   **근무 요일**: 월요일 ~ 금요일\n",
      "-   **소개**: 15년 경력의 베테랑 주방장인 김철수 셰프는 Bella Roma의 주방을 총괄하며, 모든 메뉴의 맛과 품질을 책임지고 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 이영희 (매니저)\n",
      "\n",
      "-   **역할**: 매니저\n",
      "-   **경력**: 10년\n",
      "-   **근무 요일**: 월요일 ~ 토요일\n",
      "-   **소개**: 10년 경력의 이영희 매니저는 레스토랑의 전반적인 운영, 고객 응대, 그리고 직원 관리를 담당하며 고객들이 최상의 경험을 할 수 있도록 돕습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 박민수 (서버)\n",
      "\n",
      "-   **역할**: 서버\n",
      "-   **경력**: 2년\n",
      "-   **근무 요일**: 화요일 ~ 일요일\n",
      "-   **소개**: 2년 경력의 박민수 서버는 밝은 에너지로 고객들의 식사를 돕는 홀 서빙 전문가입니다. 메뉴 추천과 고객의 편의를 책임집니다.\n",
      "\n",
      "---\n",
      "---\n",
      "# 식당 정보: Bella Roma\n",
      "\n",
      "식당은 서울 신촌에 위치한 이탈리안 레스토랑 'Bella Roma'입니다. 상세 정보는 다음과 같습니다.\n",
      "\n",
      "-   **이름**: Bella Roma\n",
      "-   **카테고리**: 이탈리안 레스토랑\n",
      "-   **위치**: 서울특별시 서대문구 신촌로 123\n",
      "-   **전화번호**: 02-123-4567\n",
      "-   **영업시간**: 매일 11:00부터 22:00까지이며, 주말에는 23:00까지 연장 운영합니다.\n",
      "-   **좌석 수**: 60석\n",
      "-   **예약**: 전화 및 온라인 예약이 가능합니다.\n",
      "-   **결제 수단**: 현금, 카드, 그리고 다양한 간편결제를 지원합니다.\n",
      "---\n",
      "# Bella Roma 메뉴 정보\n",
      "\n",
      "## 메뉴 상세 설명\n",
      "\n",
      "이 문서에서는 Bella Roma 레스토랑에서 제공하는 각 메뉴의 상세 정보를 제공합니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 마르게리타 피자\n",
      "\n",
      "-   **카테고리**: 피자\n",
      "-   **가격**: 15,000원\n",
      "-   **주재료**: 밀가루, 토마토, 치즈, 바질\n",
      "-   **설명**: 신선한 토마토 소스와 부드러운 치즈, 향긋한 바질이 어우러진 기본에 충실한 클래식 피자입니다.\n",
      "-   **알레르기 정보**: 글루텐, 유제품이 포함되어 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 까르보나라 파스타\n",
      "\n",
      "-   **카테고리**: 파스타\n",
      "-   **가격**: 18,000원\n",
      "-   **주재료**: 파스타면, 베이컨, 계란, 치즈\n",
      "-   **설명**: 고소한 베이컨과 진한 치즈, 부드러운 계란 노른자로 만든 정통 이탈리안 크림 파스타입니다.\n",
      "-   **알레르기 정보**: 글루텐, 계란, 유제품 관련 알레르기 유발 물질이 포함될 수 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 티라미수\n",
      "---\n",
      "---\n",
      "\n",
      "### 안다혜 (바리스타)\n",
      "\n",
      "-   **역할**: 바리스타\n",
      "-   **경력**: 3년\n",
      "-   **근무 요일**: 월요일 ~ 금요일\n",
      "-   **소개**: 3년 경력의 안다혜 바리스타는 Bella Roma의 모든 음료를 담당합니다. 특히, 식사와 완벽한 조화를 이루는 커피를 제공합니다.\n",
      "---\n",
      "# Bella Roma 공급업체 정보\n",
      "\n",
      "## 협력업체 소개\n",
      "\n",
      "이 문서에서는 Bella Roma 레스토랑에 신선한 재료를 공급하는 주요 협력업체 정보를 안내합니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 신촌 농산물 유통센터\n",
      "\n",
      "-   **공급 품목**: 채소, 바질\n",
      "-   **연락처**: 02-222-3333\n",
      "-   **소개**: 레스토랑에 사용되는 대부분의 신선한 채소와 향긋한 바질을 공급하는 곳입니다. 매일 아침 가장 신선한 재료를 직접 배송받아 요리에 사용합니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 이탈리아 직수입상\n",
      "\n",
      "-   **공급 품목**: 올리브오일\n",
      "-   **연락처**: 070-9876-5432\n",
      "-   **소개**: 이탈리아 현지에서 직접 수입한 최고급 엑스트라 버진 올리브오일을 전문적으로 공급합니다. 모든 요리의 풍미를 한층 더 깊게 만들어주는 핵심 재료입니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 한국 낙농 협회\n",
      "\n",
      "-   **공급 품목**: 치즈, 우유\n",
      "-   **연락처**: 02-555-1212\n",
      "-   **소개**: 피자와 파스타에 사용되는 신선한 치즈와 우유를 공급받는 공식 협력업체입니다. 엄격한 품질 관리를 통해 최상의 유제품을 제공합니다.\n",
      "\n",
      "RAG 답변: Bella Roma의 영업시간은 매일 11:00부터 22:00까지이며, 주말에는 23:00까지 연장 운영합니다.\n"
     ]
    }
   ],
   "source": [
    "def rag_pipeline(question: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Vector DB (ChromaDB)를 사용하여 비정형 데이터 기반 RAG 파이프라인을 LCEL로 실행합니다.\n",
    "    \"\"\"\n",
    "    # 1. Vector DB 초기화 및 임베딩 함수 설정\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vector_db = initialize_chroma_db(embeddings, collection_name=\"unified_data\")\n",
    "    \n",
    "    if vector_db is None:\n",
    "        return \"ERROR: Vector DB를 초기화할 수 없습니다.\"\n",
    "\n",
    "    # 2. Retriever 및 Prompt Template 정의\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "    prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "    \n",
    "    docs = retriever.invoke(question)\n",
    "        \n",
    "    # 검색된 문서 내용을 하나의 문자열로 결합\n",
    "    context = \"\\n---\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # 3. LCEL 체인 구성 (RunnablePassthrough 사용)\n",
    "    # 체인 흐름: \n",
    "    # {context: 검색(retriever), question: 질문 원문(RunnablePassthrough)} \n",
    "    # -> Prompt -> LLM -> 결과 파싱\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # 4. 질문 실행 및 결과 생성\n",
    "    try:\n",
    "        # invoke 메서드는 LCEL 체인을 실행하며, 입력은 question 문자열입니다.\n",
    "        result = rag_chain.invoke(question)\n",
    "        return context, result\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: RAG 파이프라인 실행 오류: {e}\"\n",
    "\n",
    "# 예시 사용법 (실제 실행 시 주석 해제)\n",
    "question = \"Bella Roma 레스토랑의 영업시간을 알려주세요\"\n",
    "# question = \"Bella Roma 레스토랑의 영업시간을 알려주세요.\"\n",
    "# llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0) # llm 객체 초기화 필요\n",
    "context, answer = rag_pipeline(question, llm)\n",
    "print(f\"RAG Context: {context}\")\n",
    "print()\n",
    "print(f\"RAG 답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1d4f8",
   "metadata": {},
   "source": [
    "### Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ba2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_neo4j_graph() -> Neo4jGraph:\n",
    "    \"\"\" Neo4jGraph 객체를 초기화합니다. \"\"\"\n",
    "    # 실제 환경에서는 NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD를 사용해야 합니다.\n",
    "    try:\n",
    "        # 임시 URL/인증 정보를 가정하거나 환경 변수에서 로드합니다.\n",
    "        graph = Neo4jGraph(\n",
    "            url=\"bolt://52.3.233.24:7687\", # 실제 URL로 변경 필요\n",
    "            username=\"neo4j\", \n",
    "            password=\"canvases-return-armaments\"\n",
    "        )\n",
    "        # 스키마가 미리 로드되어 있다고 가정\n",
    "        return graph\n",
    "    except Exception as e:\n",
    "        print(f\"Neo4j 초기화 오류: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320056ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faeqs\\AppData\\Local\\Temp\\ipykernel_14944\\2560474760.py:6: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph = Neo4jGraph(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRAPH RAG 답변: 마르게리타 피자는 밀가루, 토마토, 치즈, 바질을 포함하며, 유제품 알러지를 유발합니다.\n"
     ]
    }
   ],
   "source": [
    "def graph_rag_pipeline(question: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Graph DB (Neo4j)를 사용하여 스키마 기반 Cypher 쿼리 생성 파이프라인을 실행합니다.\n",
    "    \"\"\"\n",
    "    # 1. Neo4j Graph 초기화\n",
    "    graph = initialize_neo4j_graph()\n",
    "    if graph is None:\n",
    "        return \"ERROR: Neo4j Graph를 초기화할 수 없습니다.\"\n",
    "    \n",
    "    # 2. GraphCypherQAChain 생성\n",
    "    # LLM에 스키마를 전달하여 Cypher 쿼리 생성 및 실행 후 답변 생성\n",
    "    qa_chain = GraphCypherQAChain.from_llm(\n",
    "        llm=llm,\n",
    "        graph=graph,\n",
    "        verbose=False,\n",
    "        allow_dangerous_requests=True,\n",
    "        # LLM이 쿼리를 생성하고 결과를 자연어로 해석할 때 스키마가 Context로 사용됨\n",
    "    )\n",
    "    \n",
    "    # 3. 질문 실행 및 결과 생성\n",
    "    try:\n",
    "        result = qa_chain.invoke({\"query\": question})\n",
    "        return result[\"result\"]\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: GRAPH RAG 파이프라인 실행 오류: {e}\"\n",
    "\n",
    "# # 예시 사용법 (실제 실행 시 주석 해제)\n",
    "question = \"마르게리타 피자는 어떤 재료를 포함하며, 유제품 알러지를 유발하나요?\"\n",
    "answer = graph_rag_pipeline(question, llm)\n",
    "print(f\"GRAPH RAG 답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa78f6f",
   "metadata": {},
   "source": [
    "### Hybrid (RAG & Graph RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81265d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYBRID_QA_TEMPLATE = \"\"\"\n",
    "당신은 Bella Roma 레스토랑 데이터 분석 전문가입니다.\n",
    "아래에 제공된 두 가지 검색 결과 (Graph DB Context 및 Vector DB Context)를 모두 활용하여 사용자의 질문에 대한 가장 정확하고 포괄적인 답변을 한국어로 생성하십시오.\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "--- Graph DB Context ---\n",
    "{graph_context}\n",
    "---------------------------------------\n",
    "\n",
    "--- Vector DB Context ---\n",
    "{vector_context}\n",
    "-----------------------------------------\n",
    "\n",
    "# 응답 가이드라인:\n",
    "- 두 Context를 종합하여 답변하며, 충돌하는 정보가 있을 경우 Graph DB의 사실 정보를 우선하세요.\n",
    "- 답변은 전문적이고 유익한 톤을 사용하세요.\n",
    "- 두 Context 모두에 답변할 정보가 없는 경우, '제공된 정보로는 답변을 통합할 수 없습니다.'라고 명확히 언급하세요.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14df58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYBRID_QA_PROMPT = ChatPromptTemplate.from_template(HYBRID_QA_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chroma_db(embedding_function: OpenAIEmbeddings, collection_name: str) -> Optional[Chroma]:\n",
    "    \"\"\" ChromaDB 벡터 스토어를 초기화합니다. \"\"\"\n",
    "    try:\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=CHROMA_DB_PATH,\n",
    "            embedding_function=embedding_function,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        # print(f\"연결된 컬렉션({collection_name}) 문서 개수: {getattr(vector_db, '_collection').count()}\")\n",
    "        return vector_db\n",
    "    except Exception as e:\n",
    "        print(f\"ChromaDB 초기화 오류: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f51761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_neo4j_graph() -> Optional[Neo4jGraph]:\n",
    "    \"\"\" Neo4jGraph 객체를 초기화합니다. \"\"\"\n",
    "    try:\n",
    "        graph = Neo4jGraph(url=URL, username=USERNAME, password=PASSWORD)\n",
    "        return graph\n",
    "    except Exception as e:\n",
    "        print(f\"Neo4j 초기화 오류: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8096cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_vector_context(question: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\" Vector DB에서 관련 문서의 Context를 검색합니다. \"\"\"\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        vector_db = initialize_chroma_db(embeddings, collection_name=\"unified_data\")\n",
    "        \n",
    "        if vector_db is None:\n",
    "            return \"ERROR: Vector DB 초기화 실패\"\n",
    "        \n",
    "        # 5개 문서를 검색하여 텍스트 Context로 변환\n",
    "        retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "        prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "        \n",
    "        rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        docs = retriever.invoke(question)\n",
    "        \n",
    "        # 검색된 문서 내용을 하나의 문자열로 결합\n",
    "        context = \"\\n---\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        result = rag_chain.invoke(question)\n",
    "        \n",
    "        return context, result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: Vector DB 검색 오류: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d212e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca872ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph RAG - Cypher 생성을 위한 레스토랑 데이터베이스 특화 프롬프트\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Task: Generate Cypher statement to question a restaurant and customer review graph database (focused on Bella Roma).\n",
    "Instructions:\n",
    "- Use only the provided node labels, relationship types, and properties in the schema.\n",
    "- Do not use any relationship types or properties not specified in the schema.\n",
    "- Focus on extracting meaningful insights from restaurant, menu, purchase, and review data.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Note: \n",
    "- Provide only the Cypher statement.\n",
    "- Do not include explanations or apologies.\n",
    "- Generate precise, relevant Cypher queries.\n",
    "\n",
    "Examples:\n",
    "# 특정 직원의 근무일과 역할 조회\n",
    "MATCH (e:Employee)\n",
    "WHERE e.name = '김철수'\n",
    "RETURN e.name, e.role, e.workingDays\n",
    "\n",
    "# 특정 고객의 총 구매 횟수와 최고 구매액 조회\n",
    "MATCH (c:Customer)<-[:BY_CUSTOMER]-(p:Purchase)\n",
    "WHERE c.name = '사용자11'\n",
    "RETURN c.name, COUNT(p) AS totalPurchases, MAX(p.totalPrice) AS maxPurchaseAmount\n",
    "\n",
    "# 특정 메뉴의 재료와 유발 알러지 조회\n",
    "MATCH (m:MenuItem {{name: '까르보나라 파스타'}})-[:CONTAINS_INGREDIENT]->(i:Ingredient)\n",
    "OPTIONAL MATCH (m)-[:MAY_TRIGGER_ALLERGY]->(a:Allergy)\n",
    "RETURN m.name, COLLECT(i.name) AS Ingredients, COLLECT(a.name) AS Allergies\n",
    "\n",
    "# 가장 많이 팔린 메뉴 항목과 판매 수량 조회\n",
    "MATCH (m:MenuItem)<-[:FOR_MENU_ITEM]-(p:Purchase)\n",
    "RETURN m.name, SUM(p.quantity) AS TotalQuantity\n",
    "ORDER BY TotalQuantity DESC\n",
    "LIMIT 5\n",
    "\n",
    "# 4점 이상 긍정적 리뷰를 가장 많이 남긴 고객 조회\n",
    "MATCH (c:Customer)<-[:WRITTEN_BY]-(r:CustomerReview)\n",
    "WHERE r.rating >= 4\n",
    "RETURN c.name, COUNT(r) AS PositiveReviewCount\n",
    "ORDER BY PositiveReviewCount DESC\n",
    "LIMIT 10\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "\n",
    "# 결과 처리를 위한 레스토랑 QA 프롬프트\n",
    "QA_TEMPLATE = \"\"\"\n",
    "당신은 Bella Roma 레스토랑 데이터 분석 전문가로, 메뉴, 고객 행동 및 운영 정보에 대한 명확하고 간결한 정보를 한국어로 제공합니다.\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "[검색 결과 (Cypher 쿼리 실행 결과)]\n",
    "{context}\n",
    "\n",
    "# 응답 가이드라인:\n",
    "- 검색 결과에서 핵심 정보를 요약하세요\n",
    "- 레스토랑 데이터에 대한 명확하고 객관적인 개요를 제공하세요\n",
    "- 전문적이고 유익한 톤을 사용하세요\n",
    "- 고객 행동, 인기 메뉴, 운영 효율성 측면에서 중요한 패턴이나 트렌드를 강조하세요\n",
    "- 맥락이 불충분한 경우 더 많은 정보가 필요하다고 명확히 언급하세요\n",
    "- 추측이나 개인적인 해석은 피하세요\n",
    "\n",
    "# 응답 형식:\n",
    "- 간략한 발견 요약으로 시작하세요\n",
    "- 여러 항목이 발견된 경우 (예: 메뉴 목록, 고객 목록) 간결한 개요를 제공하세요\n",
    "- 가독성을 위해 글머리 기호나 짧은 단락을 사용하세요\n",
    "- 가격, 수량, 평점, 날짜와 같은 관련 수치 정보를 이해하기 쉬운 언어로 번역하세요\n",
    "\n",
    "# 예시 응답 구조:\n",
    "\"분석 결과, [주요 발견 요약]\n",
    "\n",
    "주요 특징/상세 정보:\n",
    "- [첫 번째 중요 인사이트]\n",
    "- [두 번째 중요 인사이트]\n",
    "\n",
    "추가 정보: [필요한 경우 추가 설명]\"\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "# PromptTemplate 객체 생성\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], \n",
    "    template=CYPHER_GENERATION_TEMPLATE)\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"], \n",
    "    template=QA_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "graph = initialize_neo4j_graph()\n",
    "# GraphCypherQAChain 생성 및 실행\n",
    "qa_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    qa_prompt=QA_PROMPT,\n",
    "    input_key=\"question\",  \n",
    "    output_key=\"result\",\n",
    "    # graph.schema를 cypher_prompt에 schema로 전달\n",
    "    prompt_kwargs={\"schema\": graph.schema}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea628c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"2024년 상반기(1~6월)에 긍정적인 리뷰(4점 이상)를 남긴 고객들이 가장 많이 주문한 메뉴와 메뉴 카테고리는 무엇인가요?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd7c90bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (r:Restaurant {name: 'Bella Roma'})<-[:REVIEWS_RESTAURANT]-(rev:CustomerReview)-[:WRITTEN_BY]->(c:Customer)\n",
      "WHERE rev.rating >= 4\n",
      "  AND rev.reviewDate >= date('2024-01-01') AND rev.reviewDate <= date('2024-06-30')\n",
      "WITH COLLECT(DISTINCT c) AS customers, r\n",
      "CALL {\n",
      "  WITH customers, r\n",
      "  UNWIND customers AS cust\n",
      "  MATCH (cust)<-[:BY_CUSTOMER]-(p:Purchase)-[:FOR_MENU_ITEM]->(m:MenuItem)-[:IN_MENU_CATEGORY]->(mc:MenuCategory), (p)-[:AT_RESTAURANT]->(r)\n",
      "  RETURN m.name AS menuItem, mc.name AS menuCategory, SUM(p.quantity) AS qty\n",
      "  ORDER BY qty DESC\n",
      "  LIMIT 1\n",
      "}\n",
      "CALL {\n",
      "  WITH customers, r\n",
      "  UNWIND customers AS cust\n",
      "  MATCH (cust)<-[:BY_CUSTOMER]-(p:Purchase)-[:FOR_MENU_ITEM]->(m:MenuItem)-[:IN_MENU_CATEGORY]->(mc:MenuCategory), (p)-[:AT_RESTAURANT]->(r)\n",
      "  RETURN mc.name AS topCategory, SUM(p.quantity) AS categoryQty\n",
      "  ORDER BY categoryQty DESC\n",
      "  LIMIT 1\n",
      "}\n",
      "RETURN menuItem AS TopMenuItem, menuCategory AS TopMenuItemCategory, qty AS TopMenuItemQty, topCategory AS TopCategory, categoryQty AS TopCategoryQty\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (customers, r) { ... }} {position: line: 5, column: 1, offset: 261} for query: \"MATCH (r:Restaurant {name: 'Bella Roma'})<-[:REVIEWS_RESTAURANT]-(rev:CustomerReview)-[:WRITTEN_BY]->(c:Customer)\\nWHERE rev.rating >= 4\\n  AND rev.reviewDate >= date('2024-01-01') AND rev.reviewDate <= date('2024-06-30')\\nWITH COLLECT(DISTINCT c) AS customers, r\\nCALL {\\n  WITH customers, r\\n  UNWIND customers AS cust\\n  MATCH (cust)<-[:BY_CUSTOMER]-(p:Purchase)-[:FOR_MENU_ITEM]->(m:MenuItem)-[:IN_MENU_CATEGORY]->(mc:MenuCategory), (p)-[:AT_RESTAURANT]->(r)\\n  RETURN m.name AS menuItem, mc.name AS menuCategory, SUM(p.quantity) AS qty\\n  ORDER BY qty DESC\\n  LIMIT 1\\n}\\nCALL {\\n  WITH customers, r\\n  UNWIND customers AS cust\\n  MATCH (cust)<-[:BY_CUSTOMER]-(p:Purchase)-[:FOR_MENU_ITEM]->(m:MenuItem)-[:IN_MENU_CATEGORY]->(mc:MenuCategory), (p)-[:AT_RESTAURANT]->(r)\\n  RETURN mc.name AS topCategory, SUM(p.quantity) AS categoryQty\\n  ORDER BY categoryQty DESC\\n  LIMIT 1\\n}\\nRETURN menuItem AS TopMenuItem, menuCategory AS TopMenuItemCategory, qty AS TopMenuItemQty, topCategory AS TopCategory, categoryQty AS TopCategoryQty\"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (customers, r) { ... }} {position: line: 13, column: 1, offset: 565} for query: \"MATCH (r:Restaurant {name: 'Bella Roma'})<-[:REVIEWS_RESTAURANT]-(rev:CustomerReview)-[:WRITTEN_BY]->(c:Customer)\\nWHERE rev.rating >= 4\\n  AND rev.reviewDate >= date('2024-01-01') AND rev.reviewDate <= date('2024-06-30')\\nWITH COLLECT(DISTINCT c) AS customers, r\\nCALL {\\n  WITH customers, r\\n  UNWIND customers AS cust\\n  MATCH (cust)<-[:BY_CUSTOMER]-(p:Purchase)-[:FOR_MENU_ITEM]->(m:MenuItem)-[:IN_MENU_CATEGORY]->(mc:MenuCategory), (p)-[:AT_RESTAURANT]->(r)\\n  RETURN m.name AS menuItem, mc.name AS menuCategory, SUM(p.quantity) AS qty\\n  ORDER BY qty DESC\\n  LIMIT 1\\n}\\nCALL {\\n  WITH customers, r\\n  UNWIND customers AS cust\\n  MATCH (cust)<-[:BY_CUSTOMER]-(p:Purchase)-[:FOR_MENU_ITEM]->(m:MenuItem)-[:IN_MENU_CATEGORY]->(mc:MenuCategory), (p)-[:AT_RESTAURANT]->(r)\\n  RETURN mc.name AS topCategory, SUM(p.quantity) AS categoryQty\\n  ORDER BY categoryQty DESC\\n  LIMIT 1\\n}\\nRETURN menuItem AS TopMenuItem, menuCategory AS TopMenuItemCategory, qty AS TopMenuItemQty, topCategory AS TopCategory, categoryQty AS TopCategoryQty\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'TopMenuItem': '까르보나라 파스타', 'TopMenuItemCategory': '파스타', 'TopMenuItemQty': 139, 'TopCategory': '파스타', 'TopCategoryQty': 139}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7d49d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '2024년 상반기(1~6월)에 긍정적인 리뷰(4점 이상)를 남긴 고객들이 가장 많이 주문한 메뉴와 메뉴 카테고리는 무엇인가요?',\n",
       " 'result': '분석 결과, 2024년 상반기(1~6월)에 긍정적 리뷰(4점 이상)를 남긴 고객들이 가장 많이 주문한 메뉴는 \"까르보나라 파스타\"이며, 같은 기간 최다 주문 카테고리는 \"파스타\"입니다. 집계된 주문 수는 각각 139건입니다.\\n\\n주요 특징/상세 정보:\\n- 최다 주문 메뉴: 까르보나라 파스타 — 139건\\n- 최다 주문 카테고리: 파스타 — 139건\\n- 주목할 점: 제공된 결과에서 TopMenuItemQty(139)와 TopCategoryQty(139)가 동일하게 보고되어 있습니다. 이는 본 결과 집계에서 해당 메뉴의 주문 수가 그 카테고리의 집계 수와 일치함을 의미합니다(추가 데이터 없이 그 이유를 단정할 수는 없습니다).\\n\\n운영·고객 행동 관점에서 고려할 점(관찰 기반, 추가 검증 권장):\\n- 고객 만족도가 높은 그룹에서 파스타, 특히 까르보나라의 선호도가 두드러집니다. 이는 메뉴 강조, 추천 상품 배치, 관련 재고·재료 확보 우선순위에 영향을 줄 수 있습니다.\\n- 긍정 리뷰와 특정 메뉴의 높은 상관성은 마케팅(예: 리뷰 기반 추천) 또는 교차판매 기회로 연결할 수 있는 단서가 됩니다.\\n- 주방 준비·재고 측면에서는 상위 인기 메뉴에 대한 안정적 공급·표준화가 운영 효율성 제고에 도움이 됩니다.\\n\\n필요한 추가 정보(맥락 보완 시 더 정확한 해석·조치 가능):\\n- 동일 기간 내 긍정 리뷰(4점 이상) 전체 주문 건수 및 전체 긍정 리뷰 수(비율 산출용)\\n- 다른 메뉴 및 카테고리의 순위별 집계(비중 파악)\\n- 월별 분해(1~6월 추이), 방문 채널(배달/포장/매장), 단골 여부 등 세분화 데이터\\n- 매출액·평균 결제액(해당 메뉴의 매출 기여도 분석용)\\n- 프로모션 또는 메뉴 변경이 있었는지 여부(원인 분석용)\\n\\n추가 데이터 제공이 가능하면, 비중(점유율)·추이·상관관계 분석 등을 통해 보다 구체적인 추천과 운영 개선 방안을 제시해드릴 수 있습니다.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7d103a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_graph_context(question: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\" Graph DB에서 Cypher 쿼리를 생성 및 실행하여 결과를 반환합니다. \"\"\"\n",
    "    graph = initialize_neo4j_graph()\n",
    "    if graph is None:\n",
    "        return \"ERROR: Neo4j Graph 초기화 실패\"\n",
    "    \n",
    "    # GraphCypherQAChain 생성 및 실행\n",
    "    qa_chain = GraphCypherQAChain.from_llm(\n",
    "        llm=llm,\n",
    "        graph=graph,\n",
    "        verbose=False,\n",
    "        allow_dangerous_requests=True,\n",
    "        cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "        qa_prompt=QA_PROMPT,\n",
    "        input_key=\"question\",  \n",
    "        output_key=\"result\",\n",
    "        # graph.schema를 cypher_prompt에 schema로 전달\n",
    "        prompt_kwargs={\"schema\": graph.schema}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # LLM은 Cypher 쿼리 실행 결과를 바탕으로 자연어 Context를 생성하여 반환합니다.\n",
    "        result = qa_chain.invoke({\"question\": question})\n",
    "\n",
    "        return result[\"result\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: GRAPH RAG 실행 오류: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이브리드 파이프라인\n",
    "\n",
    "def hybrid_rag_pipeline(question: str, llm: ChatOpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Graph DB (정형)와 Vector DB (비정형)의 Context를 결합하여 최종 답변을 생성합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 1. Graph DB 컨텍스트 검색 시작 ---\")\n",
    "    # (a) Graph DB 검색 - 정형 데이터 및 관계 정보\n",
    "    graph_context = fetch_graph_context(question, llm)\n",
    "    print(f\"Graph Context: {graph_context}...\")\n",
    "\n",
    "    print(\"--- 2. Vector DB 컨텍스트 검색 시작 ---\")\n",
    "    # (b) Vector DB 검색 - 비정형 텍스트 문서\n",
    "    vector_context, _ = fetch_vector_context(question, llm)\n",
    "    print(f\"Vector Context: {vector_context}...\")\n",
    "\n",
    "    # 3. Context 결합 및 최종 답변 생성 (LCEL 사용)\n",
    "    \n",
    "    # Context를 결합하는 Runnable\n",
    "    combined_context_runnable = RunnableLambda(\n",
    "        lambda q: {\n",
    "            \"question\": q, \n",
    "            \"graph_context\": graph_context, \n",
    "            \"vector_context\": vector_context\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # LCEL 체인 구성: 입력 -> Context 결합 -> Prompt -> LLM -> 결과 파싱\n",
    "    hybrid_chain = (\n",
    "        combined_context_runnable\n",
    "        | HYBRID_QA_PROMPT\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    print(\"--- 3. 하이브리드 LLM 추론 시작 ---\")\n",
    "    try:\n",
    "        final_answer = hybrid_chain.invoke(question)\n",
    "        return final_answer\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: 하이브리드 LLM 추론 오류: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c7056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Graph DB 컨텍스트 검색 시작 ---\n",
      "Graph Context: 분석 결과, 2024년 상반기(1~6월) 동안 평점 4점 이상(긍정적 리뷰)을 남긴 고객들이 가장 많이 주문한 메뉴 카테고리는 \"디저트\"이며, 총 주문 수량은 73건입니다.\n",
      "\n",
      "주요 특징/상세 정보:\n",
      "- 카테고리: 디저트\n",
      "- 총 주문 수량: 73건 (쿼리 결과의 TotalQuantity 값)\n",
      "- 분석 조건: 2024년 1월~6월, 고객 평점 ≥ 4점\n",
      "- 제공된 결과가 단일 행(디저트만)으로 반환되어, 다른 카테고리와의 비교값은 결과에 포함되어 있지 않습니다\n",
      "\n",
      "관찰 가능한 패턴/시사점:\n",
      "- 긍정적 리뷰를 남긴 고객군에서는 디저트가 가장 많이 주문된 카테고리로 나타났습니다. 이는 해당 고객 세그먼트에서 디저트의 인기 또는 자주 포함되는 품목임을 시사합니다(단, 인과관계는 확인되지 않음).\n",
      "\n",
      "추가로 확인이 필요하거나 도움이 되는 정보:\n",
      "- TotalQuantity의 정확한 정의(아이템 수 vs. 주문 라인 수)\n",
      "- 다른 메뉴 카테고리별 주문 수(비교용)\n",
      "- 긍정적 리뷰를 남긴 고유 고객 수 및 이들의 평균 주문 건수/구매금액\n",
      "- 메뉴별(품목별) 상세 주문량 및 매출 기여도\n",
      "- 월별 추이(1~6월 중 특정 월에 집중된지 여부)\n",
      "\n",
      "원하시면 위 추가 항목 중 하나를 기준으로 상세 분석(예: 품목별 상위 디저트 목록, 매출 기여도 비교, 월별 추이 등)을 실행해 드리겠습니다. 어떤 정보를 더 원하시나요?...\n",
      "--- 2. Vector DB 컨텍스트 검색 시작 ---\n",
      "Vector Context: 사용자 사용자43이 2024-04-30에 평점 4점으로 '가격은 조금 비싸지만 서비스는 만족스러워요.'라는 리뷰를 남겼습니다.\n",
      "---\n",
      "사용자 사용자34이 2024-10-26에 평점 4점으로 '직원분들이 친절해서 기분 좋게 식사했습니다.'라는 리뷰를 남겼습니다.\n",
      "---\n",
      "사용자 사용자4이 2024-09-22에 평점 3점으로 '가격은 조금 비싸지만 서비스는 만족스러워요.'라는 리뷰를 남겼습니다.\n",
      "---\n",
      "사용자 사용자34이 2024-09-25에 평점 3점으로 '가격은 조금 비싸지만 서비스는 만족스러워요.'라는 리뷰를 남겼습니다.\n",
      "---\n",
      "사용자 사용자4이 2024-12-29에 평점 5점으로 '커피가 진하고 맛있습니다.'라는 리뷰를 남겼습니다....\n",
      "--- 3. 하이브리드 LLM 추론 시작 ---\n",
      "\n",
      "=== 최종 하이브리드 RAG 답변 ===\n",
      "요약(정답)\n",
      "- 2024년 상반기(1~6월) 동안 평점 4점 이상을 남긴 고객들이 가장 많이 주문한 메뉴 카테고리는 \"디저트\"이며, 총 주문 수량은 73건입니다.\n",
      "\n",
      "근거\n",
      "- 정형 데이터(Graph DB) 분석 결과: 카테고리 = 디저트, TotalQuantity = 73 (분석 조건: 2024-01-01 ~ 2024-06-30, 고객 평점 ≥ 4점). 해당 결과는 단일 행으로 반환되어 디저트가 최다임을 나타냄(다른 카테고리의 수치는 제공되지 않음).\n",
      "- 비정형 데이터(Vector DB)에는 2024-04-30에 평점 4점을 남긴 리뷰(“가격은 조금 비싸지만 서비스는 만족스러워요.”)가 존재하나, 이 리뷰는 메뉴(디저트 등)를 특정하지 않아 추가 증거로 사용되지는 않았음.\n",
      "\n",
      "한계 및 유의사항\n",
      "- TotalQuantity의 정의(아이템 수 vs 주문 라인 수 등)가 명확하지 않습니다. 해석에 영향을 줄 수 있음.\n",
      "- 제공된 정형 결과가 디저트 단일 행만 반환되어 다른 카테고리와의 직접 비교값은 없습니다.\n",
      "- 긍정적 리뷰와 디저트 주문의 상관은 관찰되나 인과관계(디저트가 긍정평가를 유발했는지 등)는 확인되지 않습니다.\n",
      "\n",
      "원하시면 추가로 실행해 드리겠습니다 (원하시는 항목 선택):\n",
      "- 카테고리별 전체 주문 수(비교용)\n",
      "- 디저트 품목별(상세 메뉴) 상위 리스트 및 주문량\n",
      "- 긍정적 리뷰 고객의 고유 수, 평균 주문건수/평균 결제금액\n",
      "- 월별(1~6월) 디저트 주문 추이\n",
      "- TotalQuantity 정의 확인 및 재분석\n",
      "\n",
      "어떤 추가 분석을 진행할까요?\n"
     ]
    }
   ],
   "source": [
    "# 실행 예시 (사용자가 직접 실행)\n",
    "question_hybrid = \"2024년 상반기(1~6월)에 긍정적인 리뷰(4점 이상)를 남긴 고객들이 가장 많이 주문한 메뉴 카테고리는 무엇인가요?\"\n",
    "\n",
    "llm_instance = ChatOpenAI(model_name=\"gpt-5-mini\", temperature=0) \n",
    "answer = hybrid_rag_pipeline(question_hybrid, llm_instance)\n",
    "print(\"\\n=== 최종 하이브리드 RAG 답변 ===\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT Square Ontology",
   "language": "python",
   "name": "gptsquare_ontology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
